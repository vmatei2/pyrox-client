{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HYROX race analytics pyrox-client A Python client for HYROX race results that keeps analysis fast, reliable, and reproducible. Pull full races as pandas DataFrames, apply rigorous filters, and build high-signal performance models. Get started Analytics Clean race pulls Download race data straight from the CDN and cache it locally for repeatable work. Clear filters Server-side gender and division filters plus exact time-window slicing in minutes. Analysis ready Station and run splits are already renamed and normalized for modeling. Quickstart \u00b6 import pyrox client = pyrox . PyroxClient () races = client . list_races ( season = 7 ) print ( races . head ()) london = client . get_race ( season = 7 , location = \"london\" , gender = \"male\" ) print ( london [ \"total_time\" ] . describe ()) Use this documentation as a map: start with Quickstart, then move into Filtering, Data Model, and the Analytics page once you are ready to dive into your data/performance.","title":"Home"},{"location":"#quickstart","text":"import pyrox client = pyrox . PyroxClient () races = client . list_races ( season = 7 ) print ( races . head ()) london = client . get_race ( season = 7 , location = \"london\" , gender = \"male\" ) print ( london [ \"total_time\" ] . describe ()) Use this documentation as a map: start with Quickstart, then move into Filtering, Data Model, and the Analytics page once you are ready to dive into your data/performance.","title":"Quickstart"},{"location":"analytics/","text":"Analytics \u00b6 A focused set of workflows to extract signal from race data. 1. Segment z-scores \u00b6 Normalize each station split relative to the field to see strengths and weaknesses. import numpy as np import pandas as pd race = client . get_race ( season = 7 , location = \"london\" , gender = \"male\" ) split_cols = [ \"skiErg_time\" , \"sledPush_time\" , \"sledPull_time\" , \"burpeeBroadJump_time\" , \"rowErg_time\" , \"farmersCarry_time\" , \"sandbagLunges_time\" , \"wallBalls_time\" , ] field = race [ split_cols ] . astype ( float ) means = field . mean () stds = field . std ( ddof = 0 ) . replace ( 0 , np . nan ) z = ( field - means ) / stds race_z = pd . concat ([ race [[ \"name\" ]], z ], axis = 1 ) 2. Rank percentiles by total time \u00b6 race = client.get_race(season=7, location=\"london\") race = race.sort_values(\"total_time\") race[\"percentile\"] = race[\"total_time\"].rank(pct=True) 3. Create pace buckets \u00b6 bins = [0, 55, 60, 65, 70, 999] labels = [\"<55\", \"55-60\", \"60-65\", \"65-70\", \"70+\"] race[\"pace_bucket\"] = pd.cut(race[\"total_time\"], bins=bins, labels=labels) summary = race.groupby(\"pace_bucket\")[\"total_time\"].agg([\"count\", \"mean\"]) 4. Build a station share profile \u00b6 Identify which stations dominate a given athlete's time. athlete = client . get_athlete_in_race ( season = 7 , location = \"london\" , athlete_name = \"surname, name\" , ) split_cols = [ \"skiErg_time\", \"sledPush_time\", \"sledPull_time\", \"burpeeBroadJump_time\", \"rowErg_time\", \"farmersCarry_time\", \"sandbagLunges_time\", \"wallBalls_time\", ] row = athlete . iloc [ 0 ] share = row [ split_cols ] / row [ split_cols ] . sum ()","title":"Analytics"},{"location":"analytics/#analytics","text":"A focused set of workflows to extract signal from race data.","title":"Analytics"},{"location":"analytics/#1-segment-z-scores","text":"Normalize each station split relative to the field to see strengths and weaknesses. import numpy as np import pandas as pd race = client . get_race ( season = 7 , location = \"london\" , gender = \"male\" ) split_cols = [ \"skiErg_time\" , \"sledPush_time\" , \"sledPull_time\" , \"burpeeBroadJump_time\" , \"rowErg_time\" , \"farmersCarry_time\" , \"sandbagLunges_time\" , \"wallBalls_time\" , ] field = race [ split_cols ] . astype ( float ) means = field . mean () stds = field . std ( ddof = 0 ) . replace ( 0 , np . nan ) z = ( field - means ) / stds race_z = pd . concat ([ race [[ \"name\" ]], z ], axis = 1 )","title":"1. Segment z-scores"},{"location":"analytics/#2-rank-percentiles-by-total-time","text":"race = client.get_race(season=7, location=\"london\") race = race.sort_values(\"total_time\") race[\"percentile\"] = race[\"total_time\"].rank(pct=True)","title":"2. Rank percentiles by total time"},{"location":"analytics/#3-create-pace-buckets","text":"bins = [0, 55, 60, 65, 70, 999] labels = [\"<55\", \"55-60\", \"60-65\", \"65-70\", \"70+\"] race[\"pace_bucket\"] = pd.cut(race[\"total_time\"], bins=bins, labels=labels) summary = race.groupby(\"pace_bucket\")[\"total_time\"].agg([\"count\", \"mean\"])","title":"3. Create pace buckets"},{"location":"analytics/#4-build-a-station-share-profile","text":"Identify which stations dominate a given athlete's time. athlete = client . get_athlete_in_race ( season = 7 , location = \"london\" , athlete_name = \"surname, name\" , ) split_cols = [ \"skiErg_time\", \"sledPush_time\", \"sledPull_time\", \"burpeeBroadJump_time\", \"rowErg_time\", \"farmersCarry_time\", \"sandbagLunges_time\", \"wallBalls_time\", ] row = athlete . iloc [ 0 ] share = row [ split_cols ] / row [ split_cols ] . sum ()","title":"4. Build a station share profile"},{"location":"api/","text":"Client API \u00b6 PyroxClient \u00b6 Create a client: from pyrox import PyroxClient client = PyroxClient () list_races(season: int | None = None, force_refresh: bool = False) \u00b6 Return a DataFrame of available races. Filter by season when provided. get_race(...) \u00b6 get_race ( season : int , location : str , year : int | None = None , gender : str | None = None , division : str | None = None , total_time : float | tuple [ float | None , float | None ] | None = None , use_cache : bool = True , ) -> pd . DataFrame Key behaviors: - Applies server-side gender and division filters when available. - Converts time columns into minutes. - Supports strict time windows using total_time . Division values seen in the dataset include open , pro , and pro_doubles . get_athlete_in_race(...) \u00b6 get_athlete_in_race ( season : int , location : str , athlete_name : str , year : int | None = None , gender : str | None = None , division : str | None = None , use_cache : bool = True , ) -> pd . DataFrame Case-insensitive search on the name column. Raises AthleteNotFound if no match. get_season(...) \u00b6 get_season ( season : int , locations : Iterable [ str ] | None = None , gender : str | None = None , division : str | None = None , max_workers : int = 8 , use_cache : bool = True , ) -> pd . DataFrame Parallelized race fetching with a configurable worker pool. clear_cache(pattern: str = \"*\") \u00b6 Clear cached items matching a glob pattern. cache_info() -> dict \u00b6 Return cache statistics and keys.","title":"Client API"},{"location":"api/#client-api","text":"","title":"Client API"},{"location":"api/#pyroxclient","text":"Create a client: from pyrox import PyroxClient client = PyroxClient ()","title":"PyroxClient"},{"location":"api/#list_racesseason-int-none-none-force_refresh-bool-false","text":"Return a DataFrame of available races. Filter by season when provided.","title":"list_races(season: int | None = None, force_refresh: bool = False)"},{"location":"api/#get_race","text":"get_race ( season : int , location : str , year : int | None = None , gender : str | None = None , division : str | None = None , total_time : float | tuple [ float | None , float | None ] | None = None , use_cache : bool = True , ) -> pd . DataFrame Key behaviors: - Applies server-side gender and division filters when available. - Converts time columns into minutes. - Supports strict time windows using total_time . Division values seen in the dataset include open , pro , and pro_doubles .","title":"get_race(...)"},{"location":"api/#get_athlete_in_race","text":"get_athlete_in_race ( season : int , location : str , athlete_name : str , year : int | None = None , gender : str | None = None , division : str | None = None , use_cache : bool = True , ) -> pd . DataFrame Case-insensitive search on the name column. Raises AthleteNotFound if no match.","title":"get_athlete_in_race(...)"},{"location":"api/#get_season","text":"get_season ( season : int , locations : Iterable [ str ] | None = None , gender : str | None = None , division : str | None = None , max_workers : int = 8 , use_cache : bool = True , ) -> pd . DataFrame Parallelized race fetching with a configurable worker pool.","title":"get_season(...)"},{"location":"api/#clear_cachepattern-str","text":"Clear cached items matching a glob pattern.","title":"clear_cache(pattern: str = \"*\")"},{"location":"api/#cache_info-dict","text":"Return cache statistics and keys.","title":"cache_info() -&gt; dict"},{"location":"caching/","text":"Caching \u00b6 Pyrox caches results locally for speed and repeatability. Cache entries store the DataFrame plus metadata and ETags when available. Defaults \u00b6 Cache dir: ~/.cache/pyrox Manifest TTL: 2 hours Race TTL: 2 hours Season TTL: 1 hour Opt out per call \u00b6 race = client.get_race(season=7, location=\"london\", use_cache=False) Clear cache \u00b6 client.clear_cache() client.clear_cache(pattern=\"race_7_london*\") Inspect cache \u00b6 info = client.cache_info() print(info) Fields include cache_dir , total_items , total_size_mb , and cached keys.","title":"Caching"},{"location":"caching/#caching","text":"Pyrox caches results locally for speed and repeatability. Cache entries store the DataFrame plus metadata and ETags when available.","title":"Caching"},{"location":"caching/#defaults","text":"Cache dir: ~/.cache/pyrox Manifest TTL: 2 hours Race TTL: 2 hours Season TTL: 1 hour","title":"Defaults"},{"location":"caching/#opt-out-per-call","text":"race = client.get_race(season=7, location=\"london\", use_cache=False)","title":"Opt out per call"},{"location":"caching/#clear-cache","text":"client.clear_cache() client.clear_cache(pattern=\"race_7_london*\")","title":"Clear cache"},{"location":"caching/#inspect-cache","text":"info = client.cache_info() print(info) Fields include cache_dir , total_items , total_size_mb , and cached keys.","title":"Inspect cache"},{"location":"data-model/","text":"Data model \u00b6 Each race is returned as a pandas DataFrame where each row represents a single entry (athlete or doubles pair, depending on the race). Common columns \u00b6 Expect these columns in most races: name : athlete name as shown on the official results site. gender : male | female | mixed . division : open | pro | pro_doubles (case preserved as stored). total_time : total race time, minutes (float). work_time : total station time, minutes. roxzone_time : transition time, minutes. run_time : total running time, minutes. Station and run splits are normalized into readable names: skiErg_time sledPush_time sledPull_time burpeeBroadJump_time rowErg_time farmersCarry_time sandbagLunges_time wallBalls_time run1_time run2_time run3_time run4_time run5_time run6_time run7_time run8_time Time normalization \u00b6 All time columns are converted into minutes on load. Use them directly in statistical workflows without re-parsing. Schema drift \u00b6 The upstream data can evolve between seasons. Always check: print(df.columns) If a column is missing, adapt your pipeline rather than assuming it is always present.","title":"Data Model"},{"location":"data-model/#data-model","text":"Each race is returned as a pandas DataFrame where each row represents a single entry (athlete or doubles pair, depending on the race).","title":"Data model"},{"location":"data-model/#common-columns","text":"Expect these columns in most races: name : athlete name as shown on the official results site. gender : male | female | mixed . division : open | pro | pro_doubles (case preserved as stored). total_time : total race time, minutes (float). work_time : total station time, minutes. roxzone_time : transition time, minutes. run_time : total running time, minutes. Station and run splits are normalized into readable names: skiErg_time sledPush_time sledPull_time burpeeBroadJump_time rowErg_time farmersCarry_time sandbagLunges_time wallBalls_time run1_time run2_time run3_time run4_time run5_time run6_time run7_time run8_time","title":"Common columns"},{"location":"data-model/#time-normalization","text":"All time columns are converted into minutes on load. Use them directly in statistical workflows without re-parsing.","title":"Time normalization"},{"location":"data-model/#schema-drift","text":"The upstream data can evolve between seasons. Always check: print(df.columns) If a column is missing, adapt your pipeline rather than assuming it is always present.","title":"Schema drift"},{"location":"duckdb-stabilization/","text":"DuckDB Stabilization Tasks \u00b6 Define schema contracts for race_results , athletes , athlete_results , and athlete_index Add a metadata table with schema_version, build timestamp, and data source info Store the hash/version of scripts/sql_queries.py and ingest macro in metadata Validate ingest output (tables exist, required columns, non-zero row counts) Add a client-side health check for schema_version mismatch Build a minimal fixture DuckDB for tests (singles + doubles cases) Document required env vars and lock behavior (read-only vs read-write) Remote DuckDB Usage Options \u00b6 Publish the .duckdb file to object storage and download/cache locally Query Parquet directly from S3 (no DB file; requires on-the-fly views) Use a hosted DuckDB service (e.g., MotherDuck) for multi-user access Use a database server (Postgres/ClickHouse) if concurrent writes are needed Schema Contracts (v1) \u00b6 Source of truth: scripts/sql_queries.py and scripts/ingest_duckdb_from_s3.py . race_results \u00b6 Built from S3 parquet via CREATE_RACE_RESULTS . Non-nullable fields are required for stable keys and search; other fields may be NULL depending on source data. Keys and required fields: result_id VARCHAR NOT NULL (deterministic md5, primary key) season INTEGER NOT NULL location VARCHAR NOT NULL (lowercased) year INTEGER NOT NULL event_id VARCHAR NOT NULL name VARCHAR NOT NULL (trimmed) Nullable attributes: age_group VARCHAR division VARCHAR event_name VARCHAR gender VARCHAR name_raw VARCHAR nationality VARCHAR Time string columns (VARCHAR, nullable): roxzone_time run1_time , run2_time , run3_time , run4_time , run5_time , run6_time , run7_time , run8_time run_time , total_time skiErg_time , sledPush_time , sledPull_time , burpeeBroadJump_time , rowErg_time , farmersCarry_time , sandbagLunges_time , wallBalls_time work_time Time numeric columns (DOUBLE, nullable; minutes): roxzone_time_min run1_time_min , run2_time_min , run3_time_min , run4_time_min , run5_time_min , run6_time_min , run7_time_min , run8_time_min run_time_min , total_time_min skiErg_time_min , sledPush_time_min , sledPull_time_min , burpeeBroadJump_time_min , rowErg_time_min , farmersCarry_time_min , sandbagLunges_time_min , wallBalls_time_min work_time_min athletes \u00b6 Derived from race_results (canonical identity). Columns: athlete_id VARCHAR NOT NULL (md5 of canonical_name|gender|nationality, primary key) canonical_name VARCHAR NOT NULL (lower(trim(name_raw))) gender VARCHAR nationality VARCHAR athlete_results \u00b6 Link table between athletes and race results. Columns: athlete_id VARCHAR NOT NULL (FK to athletes.athlete_id) result_id VARCHAR NOT NULL (FK to race_results.result_id) link_confidence DOUBLE NOT NULL (currently 1.0 for exact match) link_method VARCHAR NOT NULL (currently \"exact_key\") Constraints: Composite primary key ( athlete_id , result_id ) athlete_index \u00b6 Pre-aggregated search index for fast lookup. Columns: athlete_id VARCHAR NOT NULL (FK to athletes.athlete_id) canonical_name VARCHAR NOT NULL name_lc VARCHAR NOT NULL (lower(canonical_name)) gender VARCHAR nationality VARCHAR race_count INTEGER NOT NULL avg_total_time DOUBLE avg_run_ratio DOUBLE Constraints: Primary key ( athlete_id ) Contract Alignment Notes \u00b6 Tests should mirror contract columns (especially athlete_results link metadata). race_results.division is required for client-side filtering in src/pyrox/reporting.py .","title":"DuckDB Stabilization Tasks"},{"location":"duckdb-stabilization/#duckdb-stabilization-tasks","text":"Define schema contracts for race_results , athletes , athlete_results , and athlete_index Add a metadata table with schema_version, build timestamp, and data source info Store the hash/version of scripts/sql_queries.py and ingest macro in metadata Validate ingest output (tables exist, required columns, non-zero row counts) Add a client-side health check for schema_version mismatch Build a minimal fixture DuckDB for tests (singles + doubles cases) Document required env vars and lock behavior (read-only vs read-write)","title":"DuckDB Stabilization Tasks"},{"location":"duckdb-stabilization/#remote-duckdb-usage-options","text":"Publish the .duckdb file to object storage and download/cache locally Query Parquet directly from S3 (no DB file; requires on-the-fly views) Use a hosted DuckDB service (e.g., MotherDuck) for multi-user access Use a database server (Postgres/ClickHouse) if concurrent writes are needed","title":"Remote DuckDB Usage Options"},{"location":"duckdb-stabilization/#schema-contracts-v1","text":"Source of truth: scripts/sql_queries.py and scripts/ingest_duckdb_from_s3.py .","title":"Schema Contracts (v1)"},{"location":"duckdb-stabilization/#race_results","text":"Built from S3 parquet via CREATE_RACE_RESULTS . Non-nullable fields are required for stable keys and search; other fields may be NULL depending on source data. Keys and required fields: result_id VARCHAR NOT NULL (deterministic md5, primary key) season INTEGER NOT NULL location VARCHAR NOT NULL (lowercased) year INTEGER NOT NULL event_id VARCHAR NOT NULL name VARCHAR NOT NULL (trimmed) Nullable attributes: age_group VARCHAR division VARCHAR event_name VARCHAR gender VARCHAR name_raw VARCHAR nationality VARCHAR Time string columns (VARCHAR, nullable): roxzone_time run1_time , run2_time , run3_time , run4_time , run5_time , run6_time , run7_time , run8_time run_time , total_time skiErg_time , sledPush_time , sledPull_time , burpeeBroadJump_time , rowErg_time , farmersCarry_time , sandbagLunges_time , wallBalls_time work_time Time numeric columns (DOUBLE, nullable; minutes): roxzone_time_min run1_time_min , run2_time_min , run3_time_min , run4_time_min , run5_time_min , run6_time_min , run7_time_min , run8_time_min run_time_min , total_time_min skiErg_time_min , sledPush_time_min , sledPull_time_min , burpeeBroadJump_time_min , rowErg_time_min , farmersCarry_time_min , sandbagLunges_time_min , wallBalls_time_min work_time_min","title":"race_results"},{"location":"duckdb-stabilization/#athletes","text":"Derived from race_results (canonical identity). Columns: athlete_id VARCHAR NOT NULL (md5 of canonical_name|gender|nationality, primary key) canonical_name VARCHAR NOT NULL (lower(trim(name_raw))) gender VARCHAR nationality VARCHAR","title":"athletes"},{"location":"duckdb-stabilization/#athlete_results","text":"Link table between athletes and race results. Columns: athlete_id VARCHAR NOT NULL (FK to athletes.athlete_id) result_id VARCHAR NOT NULL (FK to race_results.result_id) link_confidence DOUBLE NOT NULL (currently 1.0 for exact match) link_method VARCHAR NOT NULL (currently \"exact_key\") Constraints: Composite primary key ( athlete_id , result_id )","title":"athlete_results"},{"location":"duckdb-stabilization/#athlete_index","text":"Pre-aggregated search index for fast lookup. Columns: athlete_id VARCHAR NOT NULL (FK to athletes.athlete_id) canonical_name VARCHAR NOT NULL name_lc VARCHAR NOT NULL (lower(canonical_name)) gender VARCHAR nationality VARCHAR race_count INTEGER NOT NULL avg_total_time DOUBLE avg_run_ratio DOUBLE Constraints: Primary key ( athlete_id )","title":"athlete_index"},{"location":"duckdb-stabilization/#contract-alignment-notes","text":"Tests should mirror contract columns (especially athlete_results link metadata). race_results.division is required for client-side filtering in src/pyrox/reporting.py .","title":"Contract Alignment Notes"},{"location":"errors/","text":"Errors \u00b6 Pyrox exposes a small, predictable hierarchy so your pipelines can fail fast and recover gracefully. PyroxError \u00b6 Base class for client-specific errors. Catch this for broad error handling. RaceNotFound \u00b6 Raised when a season/location pair is missing from the manifest or a filter produces zero rows. AthleteNotFound \u00b6 Raised when the athlete name filter returns no matches. FileNotFoundError \u00b6 Raised when CDN reads fail unexpectedly. Consider retrying or logging the failing race metadata.","title":"Errors"},{"location":"errors/#errors","text":"Pyrox exposes a small, predictable hierarchy so your pipelines can fail fast and recover gracefully.","title":"Errors"},{"location":"errors/#pyroxerror","text":"Base class for client-specific errors. Catch this for broad error handling.","title":"PyroxError"},{"location":"errors/#racenotfound","text":"Raised when a season/location pair is missing from the manifest or a filter produces zero rows.","title":"RaceNotFound"},{"location":"errors/#athletenotfound","text":"Raised when the athlete name filter returns no matches.","title":"AthleteNotFound"},{"location":"errors/#filenotfounderror","text":"Raised when CDN reads fail unexpectedly. Consider retrying or logging the failing race metadata.","title":"FileNotFoundError"},{"location":"faq/","text":"FAQ \u00b6 Which seasons are covered? \u00b6 Historical coverage for seasons 2-7 (with season 5-6 being most used in analysis). Why are times in minutes? \u00b6 Times are normalized on load so your analysis can use numeric operations directly. How do I update cached data? \u00b6 Pass force_refresh=True to list_races or use_cache=False to any read method. Why does a race return empty? \u00b6 If a filter removes every row, RaceNotFound is raised. Try removing filters to confirm the base dataset exists.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#which-seasons-are-covered","text":"Historical coverage for seasons 2-7 (with season 5-6 being most used in analysis).","title":"Which seasons are covered?"},{"location":"faq/#why-are-times-in-minutes","text":"Times are normalized on load so your analysis can use numeric operations directly.","title":"Why are times in minutes?"},{"location":"faq/#how-do-i-update-cached-data","text":"Pass force_refresh=True to list_races or use_cache=False to any read method.","title":"How do I update cached data?"},{"location":"faq/#why-does-a-race-return-empty","text":"If a filter removes every row, RaceNotFound is raised. Try removing filters to confirm the base dataset exists.","title":"Why does a race return empty?"},{"location":"filters/","text":"Filtering \u00b6 Pyrox supports high-signal filtering at read time, so you can load only the rows you need. Gender and division \u00b6 These filters are applied server-side for efficiency. race = client.get_race( season=7, location=\"london\", gender=\"male\", # \"male\" | \"female\" | \"mixed\" division=\"open\", # \"open\" | \"pro\" | \"pro_doubles\" ) Notes: - Values are case-sensitive in the underlying parquet filter; prefer lowercase. - If the filtered dataset has no rows, Pyrox raises RaceNotFound . Total time windows \u00b6 total_time is expressed in minutes. You can pass a single value or an open interval. # Under 60 minutes sub_60 = client.get_race(season=7, location=\"london\", total_time=60) # Open interval: 50 < total_time < 60 mid_pack = client.get_race(season=7, location=\"london\", total_time=(50, 60)) # Only lower bound slow = client.get_race(season=7, location=\"london\", total_time=(70, None)) Notes: - Bounds are strict ( > and < ), not inclusive. - Filtering happens after time columns are converted to minutes.","title":"Filtering"},{"location":"filters/#filtering","text":"Pyrox supports high-signal filtering at read time, so you can load only the rows you need.","title":"Filtering"},{"location":"filters/#gender-and-division","text":"These filters are applied server-side for efficiency. race = client.get_race( season=7, location=\"london\", gender=\"male\", # \"male\" | \"female\" | \"mixed\" division=\"open\", # \"open\" | \"pro\" | \"pro_doubles\" ) Notes: - Values are case-sensitive in the underlying parquet filter; prefer lowercase. - If the filtered dataset has no rows, Pyrox raises RaceNotFound .","title":"Gender and division"},{"location":"filters/#total-time-windows","text":"total_time is expressed in minutes. You can pass a single value or an open interval. # Under 60 minutes sub_60 = client.get_race(season=7, location=\"london\", total_time=60) # Open interval: 50 < total_time < 60 mid_pack = client.get_race(season=7, location=\"london\", total_time=(50, 60)) # Only lower bound slow = client.get_race(season=7, location=\"london\", total_time=(70, None)) Notes: - Bounds are strict ( > and < ), not inclusive. - Filtering happens after time columns are converted to minutes.","title":"Total time windows"},{"location":"privacy/","text":"Privacy Policy \u00b6 Effective date: February 11, 2026 This Privacy Policy explains how Pyrox handles information when you use the Pyrox iOS app, web UI, and related API. Who operates Pyrox \u00b6 Pyrox is an independent project. The app and API are currently served from: App/web client: Pyrox UI API endpoint: https://pyrox-api.fly.dev Project repository: https://github.com/vmatei2/pyrox-client Information we process \u00b6 Pyrox is designed to work without account creation. We may process: Query inputs you provide in the app (for example athlete name searches, race filters, and report parameters). Technical request data needed to operate the service (for example IP address, user agent, request timestamps, and error logs). We do not intentionally collect sensitive personal data, payment details, or precise location data. How we use information \u00b6 We use information to: Return requested analytics and reports. Maintain service reliability, security, and performance. Investigate and fix bugs or abuse. Sharing \u00b6 We do not sell personal data. Data may be processed by infrastructure providers that host or operate the service (for example cloud hosting and networking providers) strictly to run Pyrox. Data retention \u00b6 Operational logs and request metadata are retained only as long as reasonably needed for reliability, security, and troubleshooting, then deleted or rotated. Your choices \u00b6 You can stop using the app at any time. If you have a privacy request, contact us using the project support channel below. Children's privacy \u00b6 Pyrox is not directed to children under 13, and we do not knowingly collect personal information from children under 13. Changes to this policy \u00b6 We may update this policy from time to time. Material updates will be posted on this page with a new effective date. Contact \u00b6 For privacy or support requests, use: https://github.com/vmatei2/pyrox-client/issues","title":"Privacy Policy"},{"location":"privacy/#privacy-policy","text":"Effective date: February 11, 2026 This Privacy Policy explains how Pyrox handles information when you use the Pyrox iOS app, web UI, and related API.","title":"Privacy Policy"},{"location":"privacy/#who-operates-pyrox","text":"Pyrox is an independent project. The app and API are currently served from: App/web client: Pyrox UI API endpoint: https://pyrox-api.fly.dev Project repository: https://github.com/vmatei2/pyrox-client","title":"Who operates Pyrox"},{"location":"privacy/#information-we-process","text":"Pyrox is designed to work without account creation. We may process: Query inputs you provide in the app (for example athlete name searches, race filters, and report parameters). Technical request data needed to operate the service (for example IP address, user agent, request timestamps, and error logs). We do not intentionally collect sensitive personal data, payment details, or precise location data.","title":"Information we process"},{"location":"privacy/#how-we-use-information","text":"We use information to: Return requested analytics and reports. Maintain service reliability, security, and performance. Investigate and fix bugs or abuse.","title":"How we use information"},{"location":"privacy/#sharing","text":"We do not sell personal data. Data may be processed by infrastructure providers that host or operate the service (for example cloud hosting and networking providers) strictly to run Pyrox.","title":"Sharing"},{"location":"privacy/#data-retention","text":"Operational logs and request metadata are retained only as long as reasonably needed for reliability, security, and troubleshooting, then deleted or rotated.","title":"Data retention"},{"location":"privacy/#your-choices","text":"You can stop using the app at any time. If you have a privacy request, contact us using the project support channel below.","title":"Your choices"},{"location":"privacy/#childrens-privacy","text":"Pyrox is not directed to children under 13, and we do not knowingly collect personal information from children under 13.","title":"Children's privacy"},{"location":"privacy/#changes-to-this-policy","text":"We may update this policy from time to time. Material updates will be posted on this page with a new effective date.","title":"Changes to this policy"},{"location":"privacy/#contact","text":"For privacy or support requests, use: https://github.com/vmatei2/pyrox-client/issues","title":"Contact"},{"location":"quickstart/","text":"Quickstart \u00b6 Install \u00b6 Using uv: uv pip install -e . Or from PyPI: uv pip install pyrox-client Create a client \u00b6 import pyrox client = pyrox . PyroxClient () Discover races \u00b6 races = client.list_races(season=7) print(races.head()) Load a single race \u00b6 london = client.get_race( season=7, location=\"london\", gender=\"male\", division=\"open\", ) Load a season (parallelized) \u00b6 season7 = client.get_season(season=7, locations=[\"london\", \"barcelona\"]) Pull a specific athlete \u00b6 athlete = client.get_athlete_in_race( season=7, location=\"london\", athlete_name=\"surname, name\", ) Next \u00b6 See Filtering for precise time-window queries. See Data Model to understand columns and types. See Analytics and Reproducible Research for notes of race-analysis workflows.","title":"Getting Started"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#install","text":"Using uv: uv pip install -e . Or from PyPI: uv pip install pyrox-client","title":"Install"},{"location":"quickstart/#create-a-client","text":"import pyrox client = pyrox . PyroxClient ()","title":"Create a client"},{"location":"quickstart/#discover-races","text":"races = client.list_races(season=7) print(races.head())","title":"Discover races"},{"location":"quickstart/#load-a-single-race","text":"london = client.get_race( season=7, location=\"london\", gender=\"male\", division=\"open\", )","title":"Load a single race"},{"location":"quickstart/#load-a-season-parallelized","text":"season7 = client.get_season(season=7, locations=[\"london\", \"barcelona\"])","title":"Load a season (parallelized)"},{"location":"quickstart/#pull-a-specific-athlete","text":"athlete = client.get_athlete_in_race( season=7, location=\"london\", athlete_name=\"surname, name\", )","title":"Pull a specific athlete"},{"location":"quickstart/#next","text":"See Filtering for precise time-window queries. See Data Model to understand columns and types. See Analytics and Reproducible Research for notes of race-analysis workflows.","title":"Next"},{"location":"reproducible-research/","text":"Reproducible research \u00b6 This guide documents the workflow for the notebook example_notebooks/impact_of_race_locations.ipynb . The goal is to make results repeatable, auditable, and easy to refresh. What the notebook does \u00b6 Pulls Season 7 Open Singles data. Cleans and filters timing fields. Compares event distributions for total/work time by gender. Fits a regression with event-level interaction effects. Produces plots to visualize event differences. Dependencies \u00b6 The notebook relies on: pyrox-client pandas , numpy matplotlib , seaborn statsmodels If you are using uv (recommended): uv pip install -e . If you do not already have Jupyter installed: uv pip install jupyter Run the notebook \u00b6 From the repo root: uv run jupyter notebook example_notebooks/impact_of_race_locations.ipynb Alternative: uv run jupyter lab Then open the notebook and run cells top to bottom. Data determinism \u00b6 The notebook pulls from the live CDN. To keep results stable across runs: Avoid force_refresh=True unless you intend to refresh the dataset. Keep cached data between runs. Default cache location is ~/.cache/pyrox . If you need a clean refresh, clear the cache before running: python - << 'PY' import pyrox client = pyrox . PyroxClient () client . clear_cache () PY Inputs and outputs \u00b6 Inputs: - Season 7 race data ( get_season(season=7, division=\"open\") ). - Event list in events_to_analyse . Outputs: - Plots in the event_dists/ directory (created automatically). - Regression summary in the notebook output. Repro tips \u00b6 Keep a copy of the events_to_analyse list in the notebook output so the exact event set is recorded. If you publish results, consider exporting the dataset to a dated parquet file to preserve the snapshot used for modeling.","title":"Reproducible Research"},{"location":"reproducible-research/#reproducible-research","text":"This guide documents the workflow for the notebook example_notebooks/impact_of_race_locations.ipynb . The goal is to make results repeatable, auditable, and easy to refresh.","title":"Reproducible research"},{"location":"reproducible-research/#what-the-notebook-does","text":"Pulls Season 7 Open Singles data. Cleans and filters timing fields. Compares event distributions for total/work time by gender. Fits a regression with event-level interaction effects. Produces plots to visualize event differences.","title":"What the notebook does"},{"location":"reproducible-research/#dependencies","text":"The notebook relies on: pyrox-client pandas , numpy matplotlib , seaborn statsmodels If you are using uv (recommended): uv pip install -e . If you do not already have Jupyter installed: uv pip install jupyter","title":"Dependencies"},{"location":"reproducible-research/#run-the-notebook","text":"From the repo root: uv run jupyter notebook example_notebooks/impact_of_race_locations.ipynb Alternative: uv run jupyter lab Then open the notebook and run cells top to bottom.","title":"Run the notebook"},{"location":"reproducible-research/#data-determinism","text":"The notebook pulls from the live CDN. To keep results stable across runs: Avoid force_refresh=True unless you intend to refresh the dataset. Keep cached data between runs. Default cache location is ~/.cache/pyrox . If you need a clean refresh, clear the cache before running: python - << 'PY' import pyrox client = pyrox . PyroxClient () client . clear_cache () PY","title":"Data determinism"},{"location":"reproducible-research/#inputs-and-outputs","text":"Inputs: - Season 7 race data ( get_season(season=7, division=\"open\") ). - Event list in events_to_analyse . Outputs: - Plots in the event_dists/ directory (created automatically). - Regression summary in the notebook output.","title":"Inputs and outputs"},{"location":"reproducible-research/#repro-tips","text":"Keep a copy of the events_to_analyse list in the notebook output so the exact event set is recorded. If you publish results, consider exporting the dataset to a dated parquet file to preserve the snapshot used for modeling.","title":"Repro tips"},{"location":"issues/github-actions-release/","text":"Add GitHub Actions workflow for tagging & releasing to PyPI \u00b6 Description \u00b6 Releases are currently done manually. Add a GitHub Actions workflow to build and publish when a version tag is pushed, to make releases repeatable and auditable. Scope / Tasks \u00b6 Create a workflow in .github/workflows/release.yml . Trigger on tags like v* (e.g., v1.2.3 ). Use a clean, pinned Python setup (3.12). Build with Hatch or python -m build . Publish to PyPI using TWINE_USERNAME=__token__ and TWINE_PASSWORD from repo secrets. Add minimal docs in README.md or NOTES.md describing the tagging flow: git tag vX.Y.Z git push origin vX.Y.Z Acceptance Criteria \u00b6 Pushing a version tag triggers the workflow. Builds the sdist + wheel successfully. Publishes to PyPI using secrets (no hardcoded creds). Fails clearly if tagging format is incorrect or secrets missing. Suggested Notes \u00b6 Consider adding publish permission for GitHub Actions if using trusted publishing. If using Hatch: hatch build and hatch publish should be used instead of twine .","title":"Add GitHub Actions workflow for tagging &amp; releasing to PyPI"},{"location":"issues/github-actions-release/#add-github-actions-workflow-for-tagging-releasing-to-pypi","text":"","title":"Add GitHub Actions workflow for tagging &amp; releasing to PyPI"},{"location":"issues/github-actions-release/#description","text":"Releases are currently done manually. Add a GitHub Actions workflow to build and publish when a version tag is pushed, to make releases repeatable and auditable.","title":"Description"},{"location":"issues/github-actions-release/#scope-tasks","text":"Create a workflow in .github/workflows/release.yml . Trigger on tags like v* (e.g., v1.2.3 ). Use a clean, pinned Python setup (3.12). Build with Hatch or python -m build . Publish to PyPI using TWINE_USERNAME=__token__ and TWINE_PASSWORD from repo secrets. Add minimal docs in README.md or NOTES.md describing the tagging flow: git tag vX.Y.Z git push origin vX.Y.Z","title":"Scope / Tasks"},{"location":"issues/github-actions-release/#acceptance-criteria","text":"Pushing a version tag triggers the workflow. Builds the sdist + wheel successfully. Publishes to PyPI using secrets (no hardcoded creds). Fails clearly if tagging format is incorrect or secrets missing.","title":"Acceptance Criteria"},{"location":"issues/github-actions-release/#suggested-notes","text":"Consider adding publish permission for GitHub Actions if using trusted publishing. If using Hatch: hatch build and hatch publish should be used instead of twine .","title":"Suggested Notes"},{"location":"issues/validation-helper/","text":"Add input validation for gender/division in PyroxClient \u00b6 Description \u00b6 The client currently accepts any gender / division string and passes it through filters. This can produce empty results or confusing errors when users provide invalid values. Add a small validation helper to normalize and validate these inputs early, so users get clear ValueError s. Scope / Tasks \u00b6 Define allowed values: gender: {\"male\", \"female\", \"mixed\"} division: {\"open\", \"pro\", \"pro_doubles\"} Add helper in src/pyrox/core.py : def _validate_filters(gender: Optional[str], division: Optional[str]) -> tuple[Optional[str], Optional[str]] Normalize to lowercase when not None Raise ValueError with allowed list when invalid Call helper in get_race() and get_season() before filtering Acceptance Criteria \u00b6 Invalid values raise ValueError with allowed list in message Valid values (any case) are accepted and normalized to lowercase None values still work (no filtering) No other behavior changes Suggested Tests (optional) \u00b6 Add unit tests under tests/ for: Invalid gender/division raises ValueError Valid mixed-case input is normalized and accepted","title":"Add input validation for gender/division in PyroxClient"},{"location":"issues/validation-helper/#add-input-validation-for-genderdivision-in-pyroxclient","text":"","title":"Add input validation for gender/division in PyroxClient"},{"location":"issues/validation-helper/#description","text":"The client currently accepts any gender / division string and passes it through filters. This can produce empty results or confusing errors when users provide invalid values. Add a small validation helper to normalize and validate these inputs early, so users get clear ValueError s.","title":"Description"},{"location":"issues/validation-helper/#scope-tasks","text":"Define allowed values: gender: {\"male\", \"female\", \"mixed\"} division: {\"open\", \"pro\", \"pro_doubles\"} Add helper in src/pyrox/core.py : def _validate_filters(gender: Optional[str], division: Optional[str]) -> tuple[Optional[str], Optional[str]] Normalize to lowercase when not None Raise ValueError with allowed list when invalid Call helper in get_race() and get_season() before filtering","title":"Scope / Tasks"},{"location":"issues/validation-helper/#acceptance-criteria","text":"Invalid values raise ValueError with allowed list in message Valid values (any case) are accepted and normalized to lowercase None values still work (no filtering) No other behavior changes","title":"Acceptance Criteria"},{"location":"issues/validation-helper/#suggested-tests-optional","text":"Add unit tests under tests/ for: Invalid gender/division raises ValueError Valid mixed-case input is normalized and accepted","title":"Suggested Tests (optional)"},{"location":"maintainers/","text":"Maintainer Docs \u00b6 Operational documentation for people maintaining this repository. Release workflow: docs/maintainers/release.md Reporting service and UI workflows: docs/maintainers/reporting-service.md These docs are intentionally repository-focused and may include commands that are not relevant to package users.","title":"Maintainer Docs"},{"location":"maintainers/#maintainer-docs","text":"Operational documentation for people maintaining this repository. Release workflow: docs/maintainers/release.md Reporting service and UI workflows: docs/maintainers/reporting-service.md These docs are intentionally repository-focused and may include commands that are not relevant to package users.","title":"Maintainer Docs"},{"location":"maintainers/release/","text":"Release Workflow \u00b6 This document describes how to cut and publish a new pyrox-client release. Prerequisites \u00b6 You are on an up-to-date branch (typically main ). CI is green. You have push permissions for tags. 1) Bump Version and Tag \u00b6 Use the release helper script: ./scripts/release.sh <version> Example: ./scripts/release.sh 0 .2.4 This script: updates src/pyrox/__init__.py ( __version__ ) creates a commit creates tag v<version> 2) Validate Build Artifacts Locally \u00b6 Build a wheel: uv build --wheel Validate wheel contents: python scripts/verify_wheel_contents.py This confirms the published wheel contains only the intended package surface. 3) Optional Local Quality Checks \u00b6 pytest -q ruff check . 4) Publish via GitHub Actions \u00b6 Push commit and tag: git push origin main --tags Publishing is handled by .github/workflows/release.yml . Notes \u00b6 The release workflow depends on .github/workflows/tests.yml passing first. If a bad tag was created, delete it locally and remotely before recreating: git tag -d v<version> git push --delete origin v<version>","title":"Release Workflow"},{"location":"maintainers/release/#release-workflow","text":"This document describes how to cut and publish a new pyrox-client release.","title":"Release Workflow"},{"location":"maintainers/release/#prerequisites","text":"You are on an up-to-date branch (typically main ). CI is green. You have push permissions for tags.","title":"Prerequisites"},{"location":"maintainers/release/#1-bump-version-and-tag","text":"Use the release helper script: ./scripts/release.sh <version> Example: ./scripts/release.sh 0 .2.4 This script: updates src/pyrox/__init__.py ( __version__ ) creates a commit creates tag v<version>","title":"1) Bump Version and Tag"},{"location":"maintainers/release/#2-validate-build-artifacts-locally","text":"Build a wheel: uv build --wheel Validate wheel contents: python scripts/verify_wheel_contents.py This confirms the published wheel contains only the intended package surface.","title":"2) Validate Build Artifacts Locally"},{"location":"maintainers/release/#3-optional-local-quality-checks","text":"pytest -q ruff check .","title":"3) Optional Local Quality Checks"},{"location":"maintainers/release/#4-publish-via-github-actions","text":"Push commit and tag: git push origin main --tags Publishing is handled by .github/workflows/release.yml .","title":"4) Publish via GitHub Actions"},{"location":"maintainers/release/#notes","text":"The release workflow depends on .github/workflows/tests.yml passing first. If a bad tag was created, delete it locally and remotely before recreating: git tag -d v<version> git push --delete origin v<version>","title":"Notes"},{"location":"maintainers/reporting-service/","text":"Reporting Service and UI \u00b6 This repository includes a FastAPI reporting service and a Vite/React UI for project workflows. This stack is separate from the published pyrox-client package. Canonical Backend Module \u00b6 Use: pyrox_api_service.app:app There is a compatibility shim at src/pyrox/api/ for legacy imports, but new commands should use pyrox_api_service.app:app . Backend: Local Run \u00b6 From repository root: uv pip install -e \".[api]\" export PYROX_DUCKDB_PATH = pyrox_duckdb uvicorn pyrox_api_service.app:app --reload --port 8000 Health check: curl http://localhost:8000/api/health Frontend: Local Run \u00b6 cd ui npm install VITE_API_BASE_URL = http://localhost:8000 npm run dev iOS (Capacitor) \u00b6 From ui/ : npm install npm run build npx cap add ios # first time only npm run build:cap npx cap open ios Docker and Fly.io Notes \u00b6 Docker entrypoint uses pyrox_api_service.app:app (see Dockerfile ). Fly configuration is in fly.toml . Environment variables used by the service: PYROX_DUCKDB_PATH PYROX_API_ALLOW_ORIGINS If backend code, Dockerfile, or service module paths change, redeploy the Fly app so the running image picks up updates.","title":"Reporting Service and UI"},{"location":"maintainers/reporting-service/#reporting-service-and-ui","text":"This repository includes a FastAPI reporting service and a Vite/React UI for project workflows. This stack is separate from the published pyrox-client package.","title":"Reporting Service and UI"},{"location":"maintainers/reporting-service/#canonical-backend-module","text":"Use: pyrox_api_service.app:app There is a compatibility shim at src/pyrox/api/ for legacy imports, but new commands should use pyrox_api_service.app:app .","title":"Canonical Backend Module"},{"location":"maintainers/reporting-service/#backend-local-run","text":"From repository root: uv pip install -e \".[api]\" export PYROX_DUCKDB_PATH = pyrox_duckdb uvicorn pyrox_api_service.app:app --reload --port 8000 Health check: curl http://localhost:8000/api/health","title":"Backend: Local Run"},{"location":"maintainers/reporting-service/#frontend-local-run","text":"cd ui npm install VITE_API_BASE_URL = http://localhost:8000 npm run dev","title":"Frontend: Local Run"},{"location":"maintainers/reporting-service/#ios-capacitor","text":"From ui/ : npm install npm run build npx cap add ios # first time only npm run build:cap npx cap open ios","title":"iOS (Capacitor)"},{"location":"maintainers/reporting-service/#docker-and-flyio-notes","text":"Docker entrypoint uses pyrox_api_service.app:app (see Dockerfile ). Fly configuration is in fly.toml . Environment variables used by the service: PYROX_DUCKDB_PATH PYROX_API_ALLOW_ORIGINS If backend code, Dockerfile, or service module paths change, redeploy the Fly app so the running image picks up updates.","title":"Docker and Fly.io Notes"}]}